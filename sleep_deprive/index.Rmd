---
title: "Sleep Deprivation and Software Engineering Performance"
author: "Yim Register"
date: "6/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<center>
![](../bin/images/000.png)</center>

## "Pulling an All-Nighter"
Whether you stocked up on coffee and snacks and holed up in the library to finish programming that project due tomorrow, or stayed up all night playing video games until your eyes can only squint, perhaps you are familiar with what the next day feels like after a night without sleep. Unfortunatlely, studies suggest that sleep deprivation is cognitively comparable to [being drunk](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1739867/pdf/v057p00649.pdf), with impairments to memory, reasoning, reaction time, and decision making. I think we can all agree that everyone would prefer that critical software was not made by a drunk programmer. But is it really *that bad*? Especially given the trope that programmers are up all night shouting into their gaming headsets or hacking the CIA, how bad could it really be to lose a little sleep? Answer: **bad**. It's really bad. This lesson will walk us through just how bad it is to be sleep deprived while trying to write code, and teach you how to interpret experiment design and results of an academic paper. Hopefully by the end, you'll understand statistical concepts like: **experimental design, hypothesis testing, mean, variance, standard deviation, the normal distribution, Type I and Type II errors, Bonferroni correction, correlation, Cliff's delta (effect size), and the Kruskall Wallis Test**. And hopefully you'll realize how abysmal your code will be if you forgo those extra hours to order 3am pizza.

```{r}
# there is actually A LOT to unpack here. All mean/variance/sd/normal distribution stuff should be covered before this lesson, including basic comparison between groups; perhaps in the holiday lesson
# we get a little messy when we get to nonparametric tests, effect size, and particularly the bonferroni correction. not sure how deeply I'd like to touch on that, but basically gonna say "when you start fishing around in your data, you're bound to find something significant and that's bad science. so we use this special correction to make sure it's actually *less* likely we find significance, so that we are conservative and therefore more confident in our results*

```

## Research Question

This lesson will discuss and follow the methods of [*Need for Sleep: the Impact of a Night of Sleep Deprivation on Novice Developers' Performance*.](https://arxiv.org/pdf/1805.02544.pdf) This will allow you to get practice reading academic research, while also testing out their methods in R. 

Given that we *know* sleep deprivation affects cognitive functioning, it wouldn't necessarily be the most interesting question to ask *if* sleep deprivation affects programming, but rather, *how much*? The authors form their first research question as the following:

> "To what extent does sleep deprivation impact developers’ performance?"

Immediately, you can tell that we are not going over a "Yes" or "No" response, but some kind of measure of *effect size*, and some kind of operationalization of what "performance" really is. Remember that every measure must be carefully defined; and no measure will perfectly capture a larger concept like "performance", "creativity", or "skill". Before we begin describing the methods in the paper, try to think of how *you* might measure developer performance. What would you have the participants do? How would you determine success or failure? Next, how would you check how much sleep deprivation was affecting that measure?

## Who are the participants?
Let's start by getting to know our participants, particularly between the conditions (Sleep Deprivaton vs. Regular Sleep)

```{r}
library(readxl)
library(ggplot2)
subTable <- function(data, nameCol, val){
  return (data[which(data[nameCol] == val),])

}
loadAllData <- function(fileName = "../bin/data/sleepDepr/labpackage/data/piglatin.xlsx"){

  Exp <<-   read_xlsx(fileName)
  Exp_Cleaned <<- subTable(Exp, "PVT-remove", "NO")
  SlD <<- subTable(Exp, "METHOD", "SD")
  NOSD <<- subTable(Exp, "METHOD", "RS")
  SD_Cleaned <<- subTable(SlD, "PVT-remove", "NO")
  NOSD_Cleaned <<- subTable(NOSD, "PVT-remove", "NO")
}
loadAllData()

head(Exp)

plt = ggplot(Exp,aes(METHOD,fill=METHOD))+
  geom_histogram(stat="count")+
  ggtitle("Distribution of Participants to Conditions")+
  theme_bw()
plt

post <- read_xlsx("../bin/data/sleepDepr/labpackage/data/post-questionnaire.xlsx")
data <- merge(Exp,post,by="ID")

plt = ggplot(data,aes(Age,fill=METHOD))+
  geom_histogram(position="dodge",bins = 30)+
  ggtitle("Participant Age by Condition")+
  theme_bw()
plt

nas <- which(is.na(as.numeric(as.character(data$`During your education, how many years of experience did you have with the Object Oriented Paradigm?`))))


data$`During your education, how many years of experience did you have with the Object Oriented Paradigm?` <-as.numeric(as.character(data$`During your education, how many years of experience did you have with the Object Oriented Paradigm?`))

plt = ggplot(data,aes(METHOD,data$`During your education, how many years of experience did you have with the Object Oriented Paradigm?`,fill=METHOD))+
 geom_boxplot()+
  
  ggtitle("Participant Age by Condition")+
  theme_bw()
plt

plt = ggplot(data,aes(METHOD,as.numeric(as.character(data$`During your education, how many years of experience did you have with programming?`)),fill=METHOD))+
 geom_boxplot()+
  ggtitle("Participant Age by Condition")+
  theme_bw()
plt


```

## Quasi-experimental setup

In most cases, the ideal experimental conditions would involve randomly selecting participants and assigning them into conditions (in our case, **Sleep Deprivation** or **Regular Sleep**). You can imagine this might be difficult to do with undergraduate students, and difficult to enforce. So this study relies on self-selected students who volunteered to pull an all-nighter before the programming task the next day. If you were to be asked *right now* to participate in the study, could you afford to not sleep tonight? Or would you need to be in the rested group? Perhaps you have an athletic tournament coming up, or just lost sleep the other night and can't afford another one. Or maybe, you planned on staying up tonight *anyways*, so you'd be happy to go in the sleep deprivation group (it will keep you accountable to cramming for that exam, right?). Using this kind of experimental design is referred to as **"quasi-experimental"**: the populations in the various conditions were not randomly assigned, but were self-selected into. These are perfectly fine studies to run, as long as you conduct a few tests to ensure the validity of your results. Can you guess the biggest issue with a quasi-experimental setup vs. a true experiment? The potential problem is that the self-selecting groups may be inherently different on *measures other than your independent variable**. So when you think you are measuring the effects of sleep deprivation on developer performance, perhaps you already have a significant difference of GPA or programming skill between your sleepers and non-sleepers. You can imagine coming up with a list of things that might differ between those groups, that might actually affect their developer performance more than just the sleep deprivation itself.




## How to Read Scientific Papers
- things to look for
- let's look at the methods
- uncleaned vs. cleared data distinction

## Percentage of Acceptance Asserts
PAAP =#ASSERT(PASS) / #ASSERT(ALL)× 100


## Reading in the Data, going over what each piece is
```{r}

head(Exp)

ggplot(Exp,aes(GPA,fill=METHOD))+
  geom_histogram()+
  facet_wrap(~METHOD)+
  theme_bw()


ggplot(Exp,aes(METHOD,PAAP,fill=METHOD))+
  geom_violin()+
  theme_bw()

ggplot(Exp,aes(METHOD,`#EPISODES`,fill=METHOD))+
  geom_violin()+
  theme_bw()
ggplot(Exp,aes(METHOD,`%CONF`,fill=METHOD))+
  geom_violin()+
  theme_bw()

```


## Effect Size: Cliff's Delta
We've discussed how a *p-value* does not actually tell you anything about how large the effect is. With hundreds of people, we could be absolutely positive that sleep deprivation *does* affect developer performance, but we wouldn't know anything about *how much* from a p-value. People tend to think their effect is larger if they get a smaller p-value (such as .05 vs. .00001). But this is only saying the likelihood that the groups being compared are *not different*. In one case, there is a 5% chance they are actually the same, whereas in the second case, there is only a .001% chance of the groups being actually the same. However confident we are that they are actually different, we still don't know by how much they differ. In practice, you may find that some software is faster than another with a p-value of .0001. You can be *sure* that the second software is faster. But what if it is only faster by a single milisecond? Is it worth re-doing the entire system? You be the judge.

So not only do we need to know the p-value, but we also need the effect size.
```{r}

cliffs.d  <-  function(x,y) {

	r = mean(rowMeans(sign(outer(x, y, FUN="-"))))
	r = round(r,3)
	#cat("Cliff-Deelta val = ", r)

	size = "large"
	if ( 0.147 < abs(r) & abs(r) < 0.33){
		size= "small"
	}else{
		if ( 0.33 <= abs(r) & abs(r) < 0.474){
			size = "medium"
		}
	}

	if (abs(r) < 0.147)
		size = "negligible"
	return(c(cat("\n Cliff Delta ", size, " (", r, ")\n"), size, r))


}
```

```{r}
StatisticalPowerParam <- function(distribution1, nameDistribution1, distribution2, nameDistribution2, direction = "two.sided"){

#
#  parametric analysis
#

sdd <- sd(c(distribution1,distribution2))
delta <- abs(mean(distribution1) - mean(distribution2))
n = max(length(distribution1), length(distribution2))
pow <- power.t.test(n, delta, sdd, sig.level=0.05, power=NULL, type="two.sample", alternative="two.sided")
power = round(pow$power,3)
cat(" Statistical Power", power)
cat(" beta-val", 1- power, "\n")

return(power)
}

StatisticalPowerNonParam <- function(distribution1, nameDistribution1, distribution2, nameDistribution2){

#  non-parametric analysis
 M1 <- mean(distribution1)
 M2 <- mean(distribution2)
 sd1 <- sd(distribution1)
 sd2 <- sd(distribution2)
 n1 <- length(distribution1) ### sample size
 n2 <- length(distribution2) ### sample size
 n <- n1 + n2
 pval <- replicate(1000, wilcox.test(rnorm(n1,M1,sd1), rnorm(n2,M2,sd2), paired=FALSE)$p.value)
 power = round(sum(pval < 0.05)/1000,3)
 cat(" Statistical Power", power, "\n")
 cat(" beta-val", 1- power)
 return(power)
}

Control_vs_Treatment <- function(nameTreatment, treatment, nameControl, control, direction = "two.sided") {

  sTreatment <- shapiro.test(treatment)
  sControl <- shapiro.test(control)

  if (sTreatment$p.value > 0.05 && sControl$p.value > 0.05) {
    cat(" Parametric analyses allowed \n", nameTreatment, "is normal ", round(sTreatment$p.value,4), " \n",
        nameControl, " is normal ", round(sControl$p.value,4), "\n")

    # 	  #### The distributions are both normal parametric analyses can be computed.
    # 		# t-test
    a <- t.test(treatment, control, alternative = direction, paired = FALSE, exact = FALSE, correct = TRUE)
    a = round(a$p.value,3)
    # 		# ANOVA
    # 	  a <-anova(lm(dataExp$X.TUS ~ dataExp$Method))
    # 	  a = round(as.numeric(a$`Pr(>F)`[1]),3)
    effectSize <- dCohen(treatment, control, "independent")
    s <- StatisticalPowerParam(treatment, nameTreatment, control, nameControl, direction)

  }else {
    cat(" Non-parametric analyses \n", nameTreatment, ": ", round(sTreatment$p.value,4), " \n", nameControl,
        ": ", round(sControl$p.value,4))

    a <- wilcox.test(treatment, control, alternative = direction, paired = FALSE, exact = FALSE,
                     correct = TRUE)
    #a = a$p.value
    a = round(as.numeric(a[3]),3)
    effectSize <- cliffs.d(treatment,control)
    s = StatisticalPowerNonParam(treatment, nameTreatment, control, nameControl)
  }

  #descriptive stats for treatment
  t = summary(treatment)

  #descriptive stats for control
  c = summary(control)
  impr = ((t[4]-c[4])/c[4])*100
  impr = round (impr,3)
  cat("\n Mean improvement", impr,"\\%")

  cat("\n Statistical test p-value (i.e., Hnx) ", a, "\n")
  if (a < 0.05){
    cat("***  STATISTICALLY SIGNIFICANT DIFFERENCE ***\n")
  }
  cat("&", a, "&", effectSize, "&", impr, "&", s )

}

Control_vs_Treatment('SD PAAP',SD_Cleaned$PAAP,'RS PAAP',NOSD_Cleaned$PAAP) #large and significant
Control_vs_Treatment('SD #EPISODES',SD_Cleaned$`#EPISODES`,'RS # EPISODES',NOSD_Cleaned$`#EPISODES`) #medium but not significant
Control_vs_Treatment('SD %CONF',SD_Cleaned$`%CONF`,'RS %CONF',NOSD_Cleaned$`%CONF`) #small
```


## Comparing Populations

We have to think critically about the results we would expect to see. 

- we want NO DIFFERENCE for all the measures on experience that fucci collected.
- that suggests that we are actually isolating "sleep deprivation" as our independent variable, instead of some weird effect of "who can commit to sleep deprivation vs. who cannot". Luckily, fucci goes through all of that in the experience report.
- we basically want to see a bunch of non-significance for various things like GPA, experience with the IDE, experience as a developer, etc.
- sometimes you are looking for no significance to make your point!



```{r}




```


