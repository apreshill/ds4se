---
title: "How Many Repositories Are There on GitHub?"
subtitle: "An Introduction to Statistical Thinking"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# here is where I would like a block image to go for each lesson. Imagery is important to engage the learner.
```



## Have You Ever Wondered...

We come across statistical questions all the time, even if we don't immediately realize it. From "whoah, how did Amazon know I was looking for a collection of rubber ducks that look like cowboys?" to more applied questions like "When will this server run out of storage space?" or "When will this project be completed?" Fortunatley, we live in a data-driven society; with access to billions of data sources from all over the world, we should be able to answer anything, right? Data comes in all kinds of forms; heartrate data every second from your fitness watch, location check-ins from that pizza place you went to last week, transactions, clicks, views, posts, tags... It's all digital data that can tell us something about the world. But making sense of it all requires statistical thinking and data-wrangling skills. So let's go.

## How many repositories are there now? 

Consider this question. The immediate response is to go Google it; voila! You have an answer (it seems to say that there are 100 million repositiories). But how would you actually arrive at that answer? And how would you know you were correct? Similarly to a straightforward programming task, seemingly simple questions can devolve into tons of unforseen caveats, constraints, and workarounds.



### Our Data (GHTorrent)
An important thing to understand early on in your programming career is about APIs and access to data. A lot of the time, you'll get data via a .csv or .txt file and can run everything on your local machine. But when working with really large datasets, there's just no way you can run it all on your own laptop. If you wanted to use a program to count the number of repositories on GitHub, you might consider downloading all of them and keeping a counter of each new one. Well, that's _terrabytes_ of data, so it's a no-go. So for our question, we are using something called GHTorrent. GitHub itself keeps track of all the behavior happening across the site. Commits, pushes, pull requests, new repositories, issues; and you can access those things through the GitHub API when you have a query. GHTorrent holds on to all that data, through the years. So now we can get a good picture of how fast GitHub is growing over time. Below, we query the ghtorrent project on Google Big Query, where `id` is a `project id` or **repository** with a `created_at` value. Grouping by day, we see the number of projects created each day. We would be able to access other information about those repositories, like *commits, users, language, description* and more. For now, let's look at the simplest count.

```{r}
library(bigrquery)
library(ggplot2)
set_service_token("../../servicetoken.json") # you will need to get your own service token from your account. Don't share this!
project <- "gitstats" #your project name here



sql <- "select count(id) as num_projects,date(created_at) as day
                FROM [ghtorrent-bq.ght.projects]
                group by day"




data  <- query_exec(sql, project = project, useLegacySql = FALSE)

data <- data[data$day!="2016-05-01",] #crazy outlier? #like unreasonably so


plt = ggplot(data,aes(day,num_projects))+
  geom_point()+
  theme_bw()+
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
plt

data <-data[order(data$day),]
data$n <-seq.int(nrow(data)) #adding in indexing for the days, so they're numeric
data$num_projects_sum <- cumsum(data$num_projects) 


TOTAL <- sum(data$num_projects) #here's our first answer


```

### Different Strokes for Different Folks
Could it be that depending on what we investigate and take into account, our final result changes? Does a repository count if it's not code? What if it hasn't been committed to in over a year? Do we care about forked repositories? It depends on what we care about... Do we care about *active code on GitHub*? or do we care about the sheer amount of storage space being taken up on the site? If the latter is the case, do we need information about the size of the repository; maybe even the lines of code? If we care about the former, do we need to look at which files are getting committed to, and which ones aren't being touched? Let's keep track of some of our findings in a table.

```{r, message=FALSE,warning=FALSE}

library(knitr)
library(kableExtra)
library(dplyr)

Final_Result <- c(TOTAL)
Description <- c("simplest raw sum")
different_results <- data.frame(Final_Result,Description)
kable(different_results,caption="different results obtained from different setups of the problem") %>%
  kable_styling(bootstrap_options = "striped", full_width = T)

```

### Training Data and Testing Data
You may have a heard about models being "trained". Sometimes this conjures up images of a baby robot learning its ABCs. But training a model basically means building an equation or program off of some data, so that it can reliably generalize to *new* data. We try to find an equation that "fits" the data while also being flexible enough to generalize into the future. If you've heard of the term "trend line" or "line of best fit", this is the concept we will start with when discussing training and test data. On our training data, we find a good fitting trend line. In order to evaluate how good that trend line is, we can compare how it performs on our test data. The benefit of reserving test data is that we can immediately evaluate our model, instead of simply waiting for it to be deployed in the world and either failing or succeeding (trust me, that's not a good idea). So we hold on to some data where we know the **true** values, and compare to what our trained model would predict. Below, we split our data into a training and test set, with a 70/30 split. The two plots show a very similar pattern, but our training set is much denser than the test set.


```{r}

library(caTools)

set.seed(123)   #  set seed to ensure you always have same random numbers generated
sample = sample.split(data,SplitRatio = 0.70) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
train =subset(data,sample ==TRUE) # creates a training dataset named train1 with rows which are marked as TRUE
test=subset(data, sample==FALSE)


ggplot(train,aes(n,num_projects_sum))+
  geom_point(size=.2,alpha=.1)+
  ggtitle("Training Data")+
  theme_bw()

ggplot(test,aes(n,num_projects_sum))+
  geom_point(size=.2,alpha=.2)+
  ggtitle("Testing Data")+
  theme_bw()




```

## Overfitting and Underfitting
I guarantee it, you can probably fit a model to anything. It doesn't mean it's a *good* model, and it's most certainly not guaranteed to be a *useful* one. The reason why we have training and testing data is so that we can evaluate how useful our model is at predicting new information. A big danger of fitting a model to your training data is **overfitting**. All data is messy, and contains *noise*. That means, tiny dips and jumps that are just natural in the data. But we want to capture the main trends, the main patterns, in the data we have. So we want to make sure that we aren't creating models to fit those tiny dips that don't really matter. Let's zoom in on our data, and look at a *very* close fit vs. a not so great fit:


```{r}

small <- train[train$n>2100,]

underfit <- lm(num_projects_sum~poly(n,1,raw=TRUE),data=small)
overfit <- lm(num_projects_sum~poly(n,10,raw=TRUE),data=small)
just_right <- lm(num_projects_sum~poly(n,3,raw=TRUE),data=small)


pred.underfit<- predict(underfit,data.frame(n=small$n))
pred.just_right<- predict(just_right,data.frame(n=small$n))
pred.overfit<- predict(overfit,data.frame(n=small$n))


ggplot(train[train$n>2100,],aes(n,num_projects_sum))+
  geom_point(size=.2,alpha=.5)+
  geom_line(aes(n,pred.underfit),color="red")+
  geom_line(aes(n,pred.overfit),color="blue")+
  ggtitle("Overfit and Underfit")+
  theme_bw()


```

The model with more parameters sure does seem to fit the data best! You can imagine going to GitHub and announcing *"I've found the exact predictive model of how fast you are growing!"* Your fit is so great, that it captures the small little nuances of how the growth fluctuates. You must be so proud. 

Unfortunately, let's look at the extrapolated trajectory of the models we have. Extrapolating into the future, we see that the best fitting model actually starts going in a crazy direction. It wasn't capturing the main trend, and is then subject to totally bombing on new data.


```{r}
extrap<- as.data.frame(predict(overfit,data.frame(n=seq(2100,4000,1))))
extrap$n <- seq(2100,4000,1)
colnames(extrap) <- c("overfit","n")
extrap$underfit <-predict(underfit,data.frame(n=seq(2100,4000,1)))
extrap$justright <-predict(just_right,data.frame(n=seq(2100,4000,1)))


plt = ggplot(extrap,aes(n,justright))+
  geom_line(color="orange")+
  geom_line(aes(n,overfit),color="blue")+
  geom_line(aes(n,underfit),color="red")+
  theme_bw()+
  geom_vline(xintercept = 3100,linetype="dashed")+
  ylim(-1000,NA)
plt
```

Here we have the story of the Three Bears, Machine Learning style. The first equation *sort of* gets the trend (at least it's increasing?!) but really isn't capturing the nonlinearity of the data. A line can only do so much. That's an example of **underfitting**. Then, we have a 10-term polynomial that beautifully fits the data we have; too perfectly. Are *10 terms* really necessary to capture the basic trend? And beyond just being excessive, the complex model may fail miserably on future data.


## Slow Down! How can we fit a curve to this?
We are trying to understand the true trend in the world (in our case, the number of repositories on GitHub and how that is changing over time). Simply understanding trends is nice, as we might be able to infer some causal factors that help a company grow, or we may be able to show that some phenomenon is, in fact, changing over the course of several years. But the real value comes in being able to reliably predict how that trend will continue in the future. The world is filled with patterns and statistical questions, from asking how many repositories there are on GitHub to predicting who needs financial aid or how many people will go vote each year. 

Sometimes, we can simply _look_ at the data and describe the trend line; "it's linear and decreasing", "there's very little or no correlation here", "that's exponential". But things get complicated when we care about the details (which we do!), or if we are dealing in more dimensions than we can see (whoah! don't worry we will get to that in another lesson). For now, let's talk about some of the details we might care about in _our_ problem, that we need modeling to help us determine: 

- can a model even capture the regularities in the data we have?
- how consistently is the number of repos growing over time?
- is it linear growth? exponential? could the growth go back down like a sine wave? maybe it will plateau?
- if we count all the different forks, is the growth different? what if we don't consider them at all?
- the exact slope of the growth, helping us predict what will happen next year

If we can fit a model, it means that we have an equation that helps us understand the true phenomenon happening in our data. It will _never_ be perfect (you might have heard the term "noise" and that's basically describing the beautiful unpredictableness of life). But what we hope to achieve is an equation that gets us close enough to what is happening so that it is useful. So for our data, what we hope to have by the end of this is an equation that helps us map a line through the growth of repositories on GitHub. For any year, we can look up in our equation how close we got to the _actual_ number.

So where do we start? I find it really helpful to imagine the worst model you could possibly think of, and start there. Think of this like a creative exercise; what would be the _worst_ way to solve this problem? Some great terrible ideas might include:

- Always predict "7" as the answer
- Always predict the mean across all years
- Guess randomly, using different equations, until one sticks

Luckily, the field of statistics and data science has come a tad further than these methodologies. So here are some guiding questions to help us determine which model to start with.

- What is a reasonable guess at the shape for this growth? Keep in mind that simpler is often better.
- What do we know about the world that makes that shape reasonable? For instance, we often see that exponential growth tends to taper off eventually, like in the case of X (example here). We also know that there really is only so much storage space, and other constraints that affect that happens in the world. As always, we don't _just_ have a bunch of numbers we are looking at about GitHub repos; but a result of complex interworking parts from behavior in the real world.

## What kind of curve should we fit?

```{r, message=FALSE, warning=FALSE, fig.width=7}


linear_fit <- lm(data$num_projects_sum~data$n)
second_polynomial <- lm(data$num_projects_sum~poly(data$n,2,raw=TRUE))
third_polynomial <- lm(data$num_projects_sum~poly(data$n,3,raw=TRUE))
fourth_polynomial <- lm(data$num_projects_sum~poly(data$n,4,raw=TRUE))


# testing out different shapes, from linear to third-order polynomial
data$linear <- predict(linear_fit,data.frame(x=data$n))
data$second_polynomial <- predict(second_polynomial, data.frame(x=data$n)) 
data$third_polynomial <- predict(third_polynomial, data.frame(x=data$n))
data$fourth_polynomial <- predict(fourth_polynomial, data.frame(x=data$n))


#ggplot those curves
plt <- ggplot(data,aes(day,num_projects_sum))+
  geom_point(size=1)+
  ggtitle("Github repos over time")+
  geom_line(aes(n,linear),color="#8DD3C7",size=1,alpha=.6)+
  geom_line(aes(n,second_polynomial),color="#FB8072",size=1,alpha=.6)+
  geom_line(aes(n,third_polynomial),color="#3982f7",size=1)+
  geom_vline(xintercept = seq(0,3300,by=365),linetype="dashed",alpha=.2)+
  theme_bw()+
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  xlab("Time")+
  ylab("Number of repositories")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))
plt

# we can actually test if one model is significantly different at predicting the data than another!
anova(linear_fit,second_polynomial)
anova(linear_fit,third_polynomial)
anova(second_polynomial,third_polynomial)

# they're all significantly different from each other p<.001

#to test the idea behind an anova, you can imagine creating a model we know should be closer to a model we already have, and seeing if they come out as "different"
linear_close_fit <- lm(data$num_projects_sum ~ data$n-1)
data$linear_close <-predict(linear_close_fit, data.frame(x=data$n)) 

plt <- ggplot(aes(n,num_projects_sum),data=data)+
  geom_point(size=1)+
  ggtitle("Exponential Growth","test data")+
  geom_line(aes(n,linear),color="#8DD3C7",size=1)+
  geom_line(aes(n,linear_close),color="#FB8072",size=1)+
  theme_bw()
plt




```

### Data Science is About Making Decisions (Assumptions)

### The Shape
- linear, exponential, sinusoidal, power law, here's where we explore what different shapes might represent the underlying data process
- demo different shapes on the graph

### The Fit
- we want the model to accurately represent the data, while also avoiding being too complex of a model (simpler is better)
- demo some different fits using different params

### Evaluating the Model
- how do we know if our model is any good?


```{r}

```



