---
title: "When will I be done with this project?"
author: "Yim Register"
date: "7/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Inconsistency of Measurement

We have talked a bit about how it's important to properly operationalize what we measure. Not to add more to your plate, but even if we get the right *measurement*, and do the correct *statistics*, when you're studying people, the *reliability of measurement* also matters. This lesson will highlight both a software engineering question (predicting when a project will be done), and how to think of reliability of what you measure. After walking through the [*Need for Sleep*](https://github.com/rstudio-education/ds4se/blob/master/sleep_deprive/index.Rmd) lesson, you're surely realizing how careful we have to be with our experimental setup. You can imagine that the best way to know if something is "true" is to test a concept over and over again, under lots of different circumstances, and see if your results hold true. Imagine testing participants on multiple occasions, trying to not only test your hypothesis, but seeing how steady that hypothesis holds over time. All of that is ideal, of course. Most of the time, there is limited resources, limited participants, limited time, and a rush towards a deadline. But convenience should never get in the way of good science. So let's take a look at how unreliable measurements might be.

In the following study, *Inconsistency of expert judgment-based estimates of software development effort*, expert participants rated how much effort (work-hours) they estimate for sixty software projects. They unknowingly rated six of those projects *twice*, helping to answer questions about the reliability of professional judgments about software effort estimation.


This lesson is going to explore how we estimate how long software will take to finish (or your homework, for that matter). We will explore several statistical and scientific concepts:

- reliability of measurement
- generalization of results
- features that could matter
- data wrangling
- accuracy
- within-subjects testing
- mixed models

> *How consistent are software professionals’ expert
judgment-based effort estimates?*

> *Do more accurate estimators have more consistent
expert judgment-based effort estimates than less
accurate estimators?*

```{r}
#
# inconsistency_est.R,  9 Sep 17
# Data from:
# Inconsistency of Expert Judgement-based Estimates of Software Development Effort
# Stein Grimstad and Magne J{\o}rgensen
#
# Example from:
# Empirical Software Engineering using R
# Derek M. Jones

source("../bin/data/ESEUR_config.r")

library("plyr")
library("reshape2")
library("ggplot2")
```

## Data Wrangling

Whatever your problem, you will come across the need to wrangle your data into different shapes. At first, it can seem either pointless or too complicated to be worth it. I remember first hearing the terms "melt" or "longform data" and wondering if I could just avoid ever doing that. Turns out, everyone was right and it's way better to wrap your head around a good `melt` command and move on with your life. Let's get a quick briefing on **long form**, **wide form**, **reshaping**, and **melting** data.

### Original Data
```{r}

incon=read.csv(paste0(ESEUR_dir, "../bin/data/estimation/inconsistency_est.csv.xz"), as.is=TRUE)


```

### Wide Form
This data format "stretches" the results by each Task across several columns. Each task has its own column. I've even demonstrated the *even wider* version of the data, where the **Order** variable is also "stretched" across the columns, instead of being in long form. What we are left with is simply the Subject ID number and then a separate column for each subject's ratings for each Task in each Order. 
```{r}

head(incon)

wider <-reshape(incon, direction="wide", idvar=c("Subject"), timevar="Order")

head(wider)

```

### Melt
Personally, I never understood the "melt" terminology but basically we will take the wide form data and transform it to long form. I guess it kind of goes from being stretched to dripping down and melting together? Whatever helps you to think about it, here's an example of "melting" down `tasks`.
```{r}
tasks=melt(incon, measure.vars=paste0("T", 1:6),
		variable.name="Task", value.name="Estimate")
```

### Long Form
Here you can see that tasks has been melted down to now represent each Task (T1, T2, T3..) as a factor of **Task** in one column. You can also see a difference with **Subject**, as it has to be recorded *twice* per column, in order to demonstrate the *repeated measure* for each task.

```{r}
head(tasks)
```

### Reshape

Yet another reshape will allow us to manipulate the data by whichever variable we choose. In this case, I would like to plot the *First Estimate* by the *Second Estimate*. To this day, it still helps me to draw out the exact kind of dataframe I need in order to make the comparisons or plots I have visualized in my mind. I realized that I had **Order** recorded, but those two groups of **Estimate**s were not easily comparable. I needed to manipulate my data further to have a mix of long and wide form to be the most amenable for `ggplot`.
```{r}


# reshape data
tasks <-reshape(tasks, direction="wide", idvar=c("Subject", "Task"), timevar="Order")

head(tasks)


```

I'm truly not exaggerating when I once tried to avoid any and all reshaping/reformatting of my data. It was a difficult concept to wrap my head around, and I hoped I could just format my data in one way and stick to it. Turns out that it's much easier to commit to learning these tools and apply them to make your life easier when answering statistical questions. I'm also not exaggerating when I say I've come across almost every configuration of data you can imagine; *long form, wide form, half-melted-who-knows-what form*. If I can swallow my pride and learn a `melt` command, I believe you can too.

### Log Plot
Here we include a **reference line**. This is the *y==x* line, and it is describing a hypothetical scenario where each participant perfectly reliably rates each software task. On the x axis we have their first estimate for each task, and on the y axis we see the second estimate. If they were perfectly reliable, the *y==x* line would represent the data. If they are not perfectly reliable, the points will deviate from the *y==x* line. The following plot also uses **log scale**, a common technique to visualize data with a spread that would otherwise be difficult to interpret. Try it without the log transformation and see which plot is more interpretable!

```{r}
plt = ggplot(tasks,aes(Estimate.1,Estimate.2,color=Task))+
  geom_point()+
  theme_bw()+
  ggtitle("Repeated estimates of effort for six software tasks")+
  xlab("First Estimate")+
  ylab("Second Estimate")+
  geom_abline(slope=1,linetype="dashed")+
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10')
plt
#ggsave("reliability.png",plt)

```

## Within-Subjects and Mixed Modeling

We have an opportunity here to highlight *individual differences* as a factor in experimental design and statistical testing. The design from this study is *within-subjects*. That means that the things we are comparing were done by the same people, who participated in more than one trial in the task. A *between-subjects* study would have completely different populations take part in different conditions, and they would only participate in one of those conditions being compared. **Within-subjects design mandates different statistics than Between-subjects design**. 

## Correlation (Misleading)
We have explored the idea of looking at the strength of a linear relationship through correlation. It might seem intuitive that correlation between the **First Estimate** and **Second Estimate** could tell us something about rater accuracy.
TODO:
make the (false and misleading) case for correlation as accuracy
demo on perfectly correlated, but inaccurate, data
```{r}


# Correlation is a misleading method of comparing accuracy
cor.test(tasks$Estimate.1, tasks$Estimate.2)

# Percentage difference?
# This is one of those awkward cases... TODO: switch brain
# library("boot")

```

## Relative Inconsistency (RIncons)
```{r}
 

```

## Effort Estimation: Accuracy and Bias
```{r}
# Regression Models of Software Development Effort Estimation Accuracy and Bias
#
# Example from:
# Empirical Software Engineering using R
# Derek M. Jones



library("foreign")

pal_col=rainbow(2)


kitch=read.arff(paste0(ESEUR_dir, "../bin/data/estimation/82507128-kitchenham.arff.txt"))
jorg=read.csv(paste0(ESEUR_dir, "../bin/data/estimation/Regression-models.csv.xz"), as.is=TRUE)

plot(kitch$First.estimate/100, kitch$Actual.effort/100, log="xy", col=pal_col[1],
	xlim=c(5, max(kitch$First.estimate))/100,
	ylim=c(5, max(kitch$Actual.effort))/100,
	xlab="Estimate (100 hours)", ylab="Actual\n")

points(jorg$Estimated.effort/100, jorg$Actual.effort/100, col=pal_col[2])

lines(c(5, 1e5)/100, c(5, 1e5)/100, col="grey")

legend(x="bottomright", legend=c("Kitchenham", "Jørgensen"), bty="n", fill=pal_col, cex=1.2)

# 
# plot(kitch$First.estimate, kitch$Adjusted.function.points, log="xy", col=point_col,
# 	xlab="Estimate", ylab="Adjusted.function.points\n")
# 
# plot(kitch$Actual.effort, kitch$Adjusted.function.points, log="xy", col=point_col,
# 	xlab="Actual", ylab="Function points\n")
# 
# library("MASS")
# 
# e_mod=glm(Actual.effort ~ (First.estimate+Adjusted.function.points
# 				+Client.code+Project.type
# 				+First.estimate.method)^2
# 				, data=kitch)
# 
# t=stepAIC(e_mod)
# summary(t)
# 
# 
# e_mod=glm(formula = Actual.effort ~ First.estimate
#     + First.estimate:Adjusted.function.points
#     + First.estimate:Client.code
#     , data = kitch)
# 
# summary(e_mod)
```